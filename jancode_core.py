# jancode_core.py
# jancode.xyz ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨Google Sheetsæ“ä½œã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯

import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import requests
from bs4 import BeautifulSoup
import time
import socket
from urllib.parse import urljoin

class JanCodeScraper:
    """
    jancode.xyz ã‹ã‚‰JANã‚³ãƒ¼ãƒ‰æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Google Sheetsã‚’æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    BASE_URL = "https://www.jancode.xyz/"
    SEARCH_URL = "https://www.jancode.xyz/code/"
    
    def __init__(self, config, logger_callback=print):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        """
        self.config = config
        self.logger = logger_callback
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })
        self._setup_gsheets()

    def _setup_gsheets(self):
        """Google Sheetsã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹"""
        self.logger("ğŸ”§ Google Sheets ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
        try:
            scopes = ["https://www.googleapis.com/auth/spreadsheets"]
            creds = Credentials.from_service_account_file(self.config["json_path"], scopes=scopes)
            self.sheets_service = build("sheets", "v4", credentials=creds, cache_discovery=False)
            gc = gspread.authorize(creds)
            spreadsheet = gc.open_by_key(self.config["spreadsheet_id"])
            self.sheet = spreadsheet.worksheet(self.config["sheet_name"])
            self.logger("âœ… Google Sheets ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†")
        except gspread.exceptions.WorksheetNotFound:
            self.logger(f"âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: '{self.config['sheet_name']}'")
            raise
        except Exception as e:
            self.logger(f"âŒ Google Sheetsã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise

    def _column_letter_to_number(self, column_letter):
        """ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®åˆ—æ–‡å­—ã‚’æ•°å€¤ã«å¤‰æ›"""
        num = 0
        for char in column_letter.upper():
            num = num * 26 + (ord(char) - ord('A') + 1)
        return num

    def _get_detail_page_urls(self, jan_codes):
        """ä¸€æ‹¬æ¤œç´¢ã‚’è¡Œã„ã€è©³ç´°ãƒšãƒ¼ã‚¸ã®URLãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
        self.logger(f"ğŸ” {len(jan_codes)}ä»¶ã®JANã‚³ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬æ¤œç´¢ä¸­...")
        payload = {
            "q": "\n".join(jan_codes),
            "action": "search",
            "process": "code_multi"
        }
        try:
            response = self.session.post(self.SEARCH_URL, data=payload, timeout=60)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            urls = []
            for a_tag in soup.select(".result-box-out > a"):
                href = a_tag.get('href')
                if href:
                    full_url = urljoin(self.BASE_URL, href)
                    urls.append(full_url)
            
            self.logger(f"âœ… è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’{len(urls)}ä»¶å–å¾—ã—ã¾ã—ãŸã€‚")
            return urls
        except requests.exceptions.RequestException as e:
            self.logger(f"âŒ ä¸€æ‹¬æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return []

    def _scrape_detail_page(self, url):
        """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            response = self.session.get(url, timeout=60)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            info = {}
            table = soup.find('table', class_='table-block')
            if not table:
                return None

            for row in table.find_all('tr'):
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.text.strip()
                    # --- â–¼ã‚­ãƒ¼åã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨çµ±ä¸€â–¼ ---
                    if key == "å•†å“ã‚¤ãƒ¡ãƒ¼ã‚¸":
                        img_tag = td.find('img')
                        info["å•†å“ã‚¤ãƒ¡ãƒ¼ã‚¸URL"] = urljoin(self.BASE_URL, img_tag['src']) if img_tag and 'src' in img_tag.attrs else ''
                    elif key == "ä¾¡æ ¼èª¿æŸ»":
                        links = {a.find('img')['src'].split('/')[-1].split('.')[0]: a['href'] for a in td.select('a') if a.find('img')}
                        info["æ¥½å¤©URL"] = links.get('rakuten', '')
                        info["YahooURL"] = links.get('yahoo', '')
                        info["AmazonURL"] = links.get('amazon', '')
                    elif key == "JANã‚·ãƒ³ãƒœãƒ«":
                        img_tag = td.find('img')
                        info["JANã‚·ãƒ³ãƒœãƒ«ç”»åƒURL"] = urljoin(self.BASE_URL, img_tag['src']) if img_tag and 'src' in img_tag.attrs else ''
                    # --- â–²ã“ã“ã¾ã§ä¿®æ­£â–² ---
                    elif key == "å•†å“ã‚¸ãƒ£ãƒ³ãƒ«":
                        info[key] = " > ".join([a.text.strip() for a in td.find_all('a')])
                    else:
                        info[key] = td.text.strip()
            
            info["è©³ç´°ãƒšãƒ¼ã‚¸URL"] = url
            return info
        except requests.exceptions.RequestException as e:
            self.logger(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
        except Exception as e:
            self.logger(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None

    def _check_and_create_headers(self):
        """å‡ºåŠ›åˆ—ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆã™ã‚‹"""
        self.logger("ğŸ” ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèª...")
        try:
            # --- â–¼ã“ã“ã‹ã‚‰ä¿®æ­£â–¼ ---
            # GUIã§æŒ‡å®šã•ã‚ŒãŸå‡ºåŠ›é–‹å§‹åˆ—ã‚’å–å¾—
            start_col_letter = self.config['output_start_col_letter'].upper()
            start_col_num = self._column_letter_to_number(start_col_letter)

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦æœŸå¾…ã•ã‚Œã‚‹å€¤ã®ãƒªã‚¹ãƒˆï¼ˆå‡ºåŠ›åˆ—ã®ã¿ï¼‰
            expected_headers = [
                "å•†å“å", "ä¼šç¤¾å", "ä¼šç¤¾åã‚«ãƒŠ", "å•†å“ã‚¸ãƒ£ãƒ³ãƒ«", "ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—",
                "å•†å“ã‚¤ãƒ¡ãƒ¼ã‚¸URL", "JANã‚·ãƒ³ãƒœãƒ«ç”»åƒURL", "æ¥½å¤©URL", "YahooURL",
                "AmazonURL", "è©³ç´°ãƒšãƒ¼ã‚¸URL"
            ]
            
            # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®1è¡Œç›®å…¨ä½“ã®å€¤ã‚’å–å¾—
            current_headers_all = self.sheet.row_values(1)
            
            # å¿…è¦ãªé•·ã•ã«è¶³ã‚Šãªã„å ´åˆã¯ç©ºæ–‡å­—ã§åŸ‹ã‚ã‚‹
            if len(current_headers_all) < start_col_num + len(expected_headers):
                padding_needed = (start_col_num + len(expected_headers)) - len(current_headers_all)
                current_headers_all.extend([""] * padding_needed)
            
            # å®Ÿéš›ã®ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã§å–å¾—
            actual_headers_slice = current_headers_all[start_col_num - 1 : start_col_num - 1 + len(expected_headers)]

            # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæœŸå¾…é€šã‚Šã‹æ¯”è¼ƒ
            if actual_headers_slice != expected_headers:
                self.logger("â„¹ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆã¾ãŸã¯æ›´æ–°ã—ã¾ã™...")
                
                # æ›´æ–°ç¯„å›²ã‚’æ­£ã—ãæŒ‡å®š
                range_to_update = f"{start_col_letter}1"
                self.sheet.update(range_to_update, [expected_headers], value_input_option='USER_ENTERED')
                
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ/æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
            # --- â–²ã“ã“ã¾ã§ä¿®æ­£â–² ---

        except Exception as e:
            self.logger(f"âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèªãƒ»ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    def run_process(self):
        """ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹"""
        try:
            self.logger("\nğŸš€ jancode.xyz ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†é–‹å§‹ ğŸš€")
            self._check_and_create_headers()
        except Exception as e:
            self.logger(f"CRITICAL: ãƒ˜ãƒƒãƒ€ãƒ¼ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚: {e}")
            return

        current_row = self.config["start_row"]
        batch_size = self.config.get("batch_size", 100)
        
        while True:
            self.logger(f"\n--- {current_row}è¡Œç›®ã‹ã‚‰ã®ãƒãƒƒãƒå‡¦ç†ã‚’é–‹å§‹ ---")
            try:
                jan_col_num = self._column_letter_to_number(self.config["jan_col_letter"])
                jan_codes_raw = self.sheet.col_values(jan_col_num)[current_row - 1 : current_row - 1 + batch_size]
                jan_codes_to_process = [code for code in jan_codes_raw if code.strip()]
            except Exception as e:
                self.logger(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                break

            if not jan_codes_to_process:
                self.logger("â„¹ï¸ å‡¦ç†å¯¾è±¡ã®JANã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break

            detail_urls = self._get_detail_page_urls(jan_codes_to_process)
            time.sleep(self.config.get("delay", 3))

            if not detail_urls:
                current_row += len(jan_codes_raw) if jan_codes_raw else batch_size
                continue

            all_scraped_data = {}
            for url in detail_urls:
                self.logger(f"ğŸ“„ è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­: {url}")
                scraped_info = self._scrape_detail_page(url)
                if scraped_info and "ã‚³ãƒ¼ãƒ‰ç•ªå·" in scraped_info:
                    all_scraped_data[scraped_info["ã‚³ãƒ¼ãƒ‰ç•ªå·"]] = scraped_info
                time.sleep(self.config.get("delay", 3))

            if all_scraped_data:
                self.logger(f"ğŸ“ {len(all_scraped_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ã¾ã™...")
                update_requests = []
                
                for i, jan_in_sheet in enumerate(jan_codes_raw):
                    if jan_in_sheet in all_scraped_data:
                        data = all_scraped_data[jan_in_sheet]
                        row_to_write = current_row + i
                        # --- â–¼æ›¸ãè¾¼ã‚€å€¤ã®ã‚­ãƒ¼ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨çµ±ä¸€â–¼ ---
                        values = [
                            data.get("å•†å“å", ""), data.get("ä¼šç¤¾å", ""), data.get("ä¼šç¤¾åã‚«ãƒŠ", ""),
                            data.get("å•†å“ã‚¸ãƒ£ãƒ³ãƒ«", ""), data.get("ã‚³ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—", ""),
                            data.get("å•†å“ã‚¤ãƒ¡ãƒ¼ã‚¸URL", ""), data.get("JANã‚·ãƒ³ãƒœãƒ«ç”»åƒURL", ""),
                            data.get("æ¥½å¤©URL", ""), data.get("YahooURL", ""), data.get("AmazonURL", ""),
                            data.get("è©³ç´°ãƒšãƒ¼ã‚¸URL", "")
                        ]
                        # --- â–²ã“ã“ã¾ã§ä¿®æ­£â–² ---
                        range_str = f"{self.config['output_start_col_letter']}{row_to_write}"
                        update_requests.append({"range": range_str, "values": [values]})
                
                if update_requests:
                    self.sheet.batch_update(update_requests, value_input_option='USER_ENTERED')
                    self.logger("âœ… æ›¸ãè¾¼ã¿å®Œäº†ã€‚")

            current_row += len(jan_codes_raw) if jan_codes_raw else batch_size
        
        self.logger("\nğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ ğŸ‰")
